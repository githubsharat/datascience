{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102b3d7a",
   "metadata": {},
   "source": [
    "1. **Summation Junction and Threshold Activation Function**:\n",
    "   - The summation junction of a neuron takes multiple inputs, multiplies each by their respective weight, and sums them up. Essentially, it computes the weighted sum of the inputs. \n",
    "   - The threshold activation function outputs a value (usually 1) if the input (usually the sum from the summation junction) is above a certain threshold, and another value (usually 0) otherwise.\n",
    "\n",
    "2. **Step Function vs. Threshold Function**:\n",
    "   - A step function produces one value for inputs below a certain value, and another value for inputs above that value. It's a discontinuous function that \"jumps\" from one value to another.\n",
    "   - The threshold function is a specific kind of step function where the step occurs at a user-defined threshold. Essentially, every threshold function is a step function, but not every step function is a threshold function.\n",
    "\n",
    "3. **McCullochâ€“Pitts Model of Neuron**:\n",
    "   It's one of the earliest and simplest models of a neuron. This model takes binary inputs, multiplies them by weights, and passes the weighted sum through a thresholding function to produce a binary output.\n",
    "\n",
    "4. **ADALINE Network Model**:\n",
    "   ADALINE (Adaptive Linear Neuron) is a type of artificial neuron. It uses a linear activation function and is trained using the delta rule or least mean squares. The primary distinction between ADALINE and the basic perceptron is the activation function; ADALINE employs a continuous linear activation function rather than a unit step.\n",
    "\n",
    "5. **Constraint of a Simple Perceptron & Real-world Data**:\n",
    "   The simple perceptron can only model linearly separable functions. When faced with real-world datasets that aren't linearly separable, a simple perceptron will fail to converge or find a solution.\n",
    "\n",
    "6. **Linearly Inseparable Problem & Hidden Layer Role**:\n",
    "   - A linearly inseparable problem is one where the classes cannot be separated by a straight line (in 2D), or a plane (in 3D), etc.\n",
    "   - Hidden layers introduce non-linearity to a neural network, allowing it to model complex, non-linear relationships and solve linearly inseparable problems.\n",
    "\n",
    "7. **XOR Problem in Simple Perceptron**:\n",
    "   The XOR (exclusive or) function is a basic example of a linearly inseparable problem. A simple perceptron cannot model the XOR function because the true and false outputs aren't separable by a straight line.\n",
    "\n",
    "8. **Multi-layer Perceptron for A XOR B**:\n",
    "   - **Input Layer**: Two neurons, one for A and one for B.\n",
    "   - **Hidden Layer**: Two neurons. One models `A AND NOT B` and the other models `NOT A AND B`.\n",
    "   - **Output Layer**: One neuron which ORs the outputs of the two hidden layer neurons.\n",
    "\n",
    "9. **Single-layer Feed Forward Architecture of ANN**:\n",
    "   In this architecture, the neural network has only one layer which consists of weights connecting inputs directly to outputs. The neurons in this layer take the weighted sum of the inputs and pass them through an activation function. There are no hidden layers.\n",
    "\n",
    "10. **Competitive Network Architecture of ANN**:\n",
    "   In competitive networks, neurons compete among themselves to become activated (or \"fire\"). This is typically used in unsupervised learning, particularly clustering. One common method is called Winner-Takes-All, where the neuron with the highest activation fires and inhibits all other neurons.\n",
    "\n",
    "11. **Backpropagation in Multi-layer Feed Forward Neural Network**:\n",
    "   - **Forward Pass**: Input is passed through the network to compute the output.\n",
    "   - **Compute Loss**: Determine the error/loss using a loss function comparing the predicted and actual outputs.\n",
    "   - **Backward Pass**:\n",
    "     1. Compute the gradient of the loss with respect to each weight in the network using the chain rule.\n",
    "     2. Adjust weights in the opposite direction of the gradient to minimize the loss.\n",
    "   - **Iterate**: Repeat the forward and backward passes until the loss converges to a minimum value.\n",
    "\n",
    "12. **Advantages and Disadvantages of Neural Networks**:\n",
    "   **Advantages**:\n",
    "   - Ability to model complex, non-linear relationships.\n",
    "   - Ability to learn directly from data; no need for manual feature engineering.\n",
    "   - Can handle large datasets.\n",
    "\n",
    "   **Disadvantages**:\n",
    "   - Require a lot of data to train.\n",
    "   - Can be computationally intensive.\n",
    "   - Sensitive to the choice of hyperparameters.\n",
    "   - Lack of interpretability.\n",
    "\n",
    "13. **Short Notes**:\n",
    "   1. **Biological Neuron**: The fundamental unit of the brain, responsible for transmitting and processing information using electrical and chemical signals.\n",
    "   2. **ReLU Function**: Rectified Linear Unit (ReLU) is an activation function that outputs the input if it's positive and zero otherwise. It's computationally efficient and has become a default choice in many deep learning models.\n",
    "   3. **Single-layer Feed Forward ANN**: A neural network with no hidden layers. The inputs are directly connected to the outputs.\n",
    "   4. **Gradient Descent**: An optimization technique used to minimize the error in a model by iteratively adjusting the model's weights. It works by computing the gradient of the loss and moving in the opposite direction.\n",
    "   5. **Recurrent Networks**: A type of neural network where connections between neurons can form cycles. This allows them to maintain a \"memory\" of previous inputs, making them useful for sequences and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39e609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

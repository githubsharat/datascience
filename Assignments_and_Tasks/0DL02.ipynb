{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102b3d7a",
   "metadata": {},
   "source": [
    "1. **Artificial Neuron Structure**:\n",
    "   - **Inputs**: Like dendrites in biological neurons, they receive signals.\n",
    "   - **Weights**: Multipliers for each input to amplify or reduce them.\n",
    "   - **Bias**: An additional parameter added to the weighted sum to allow flexibility in the function.\n",
    "   - **Activation Function**: Determines if the neuron \"fires\" or not, akin to a biological neuron's threshold.\n",
    "   - **Output**: The result after processing, similar to the axon in a biological neuron.\n",
    "\n",
    "   The artificial neuron is inspired by a biological neuron with inputs, processing, and outputs. However, it's a mathematical approximation, and not a biological replication.\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - **Linear**: f(x) = x. Provides a constant output for a constant input.\n",
    "   - **Sigmoid**: f(x) = 1 / (1 + e^-x). Maps any input to a value between 0 and 1.\n",
    "   - **Tanh (Hyperbolic Tangent)**: f(x) = (2 / (1 + e^-2x)) - 1. Maps input values between -1 and 1.\n",
    "   - **ReLU (Rectified Linear Unit)**: f(x) = max(0, x). All negative values are zeroed.\n",
    "   - **Leaky ReLU**: f(x) = x if x > 0, else 0.01x. Allows a small gradient when the unit is not active.\n",
    "\n",
    "3. **Rosenblattâ€™s Perceptron Model**:\n",
    "   - A perceptron is a single-layer neural network.\n",
    "   - It receives multiple inputs, multiplies them by weights, and then applies an activation (typically a step function) to decide the output (either 0 or 1).\n",
    "   - To classify data, the perceptron is trained using the perceptron learning rule. If the prediction is correct, weights are unchanged. If not, weights are adjusted in the direction of the correct classification.\n",
    "\n",
    "   **Classifying with Given Weights**:\n",
    "   Use the equation `y = w0 + w1*x1 + w2*x2` and apply a step function to get the binary classification.\n",
    "\n",
    "   For (3, 4): y = (-1) + 2(3) + 1(4) = 9. If using a simple threshold step function with a threshold of 0, the classification is 1 (or true).\n",
    "\n",
    "   Similar calculations can be performed for the rest of the points.\n",
    "\n",
    "4. **Multi-layer Perceptron (MLP) & XOR Problem**:\n",
    "   MLPs have an input layer, one or more hidden layers, and an output layer. Each layer has multiple neurons. MLPs are capable of modeling non-linear relationships, unlike simple perceptrons.\n",
    "   \n",
    "   The XOR problem isn't linearly separable, meaning a single perceptron can't solve it. However, an MLP with a single hidden layer containing two neurons can solve it by creating new decision boundaries.\n",
    "\n",
    "5. **Artificial Neural Network (ANN) & Architectures**:\n",
    "   ANNs are computing systems inspired by the brain's neural networks. They process information using interconnected nodes (neurons). \n",
    "   \n",
    "   **Architectures**:\n",
    "   - **Single-layer Feedforward**: Direct flow from input to output, no hidden layers.\n",
    "   - **Multi-layer Perceptron (MLP)**: Multiple layers including hidden layers. Suitable for complex patterns.\n",
    "   - **Recurrent Neural Network (RNN)**: Connections between neurons can loop, allowing memory.\n",
    "   - **Convolutional Neural Network (CNN)**: Specialized for spatial data like images.\n",
    "   \n",
    "6. **Learning in ANN & Weight Assignment Challenge**:\n",
    "   Learning in ANNs involves adjusting the weights and biases to minimize the difference between the predicted and actual outputs. The challenge is finding the optimal set of weights for best performance, as many combinations exist. Gradient Descent is a method to iteratively adjust weights to find a local (or global) minimum in the error.\n",
    "\n",
    "7. **Backpropagation**:\n",
    "   Backpropagation is a supervised learning algorithm used to train ANNs by minimizing the error between actual and predicted outputs. \n",
    "\n",
    "   **Steps**:\n",
    "   - Propagate the input forward through the network to get the output.\n",
    "   - Calculate the error.\n",
    "   - Propagate the error backward, adjusting weights.\n",
    "\n",
    "   **Limitations**:\n",
    "   - Can get stuck in local minima.\n",
    "   - Requires tuning hyperparameters like learning rate.\n",
    "   - Computationally intensive.\n",
    "\n",
    "   Adjusting weights involves computing the gradient of the loss function concerning each weight and adjusting weights in the direction that reduces the error.\n",
    "\n",
    "8. **Short Notes**:\n",
    "   1. **Artificial Neuron**: Basic computation unit of ANNs. Takes multiple inputs, applies weights and biases, processes them, and produces an output.\n",
    "   2. **Multi-layer Perceptron**: Neural network with one or more hidden layers.\n",
    "   3. **Deep Learning**: Subset of ML using neural networks with many layers to model complex patterns.\n",
    "   4. **Learning Rate**: Hyperparameter controlling the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80b915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

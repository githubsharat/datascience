{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102b3d7a",
   "metadata": {},
   "source": [
    "This is a collection of theoretical and practical questions, so I'll address each one separately:\n",
    "\n",
    "1. **Initializing Weights**:\n",
    "   No, you shouldn't initialize all the weights to the same value, even if that value comes from a random initialization like He. If weights are identical, during backpropagation all neurons in a given layer will receive the same gradient and make the same update, essentially making them identical throughout training. This negates the very essence of deep networks, as all neurons would act the same.\n",
    "\n",
    "2. **Bias Initialization**:\n",
    "   Yes, it's common practice to initialize biases to 0. They learn their required non-zero values during training.\n",
    "\n",
    "3. **SELU vs. ReLU**:\n",
    "   - **Self-normalization**: SELU activations tend to maintain a mean close to 0 and standard deviation close to 1, which helps combat the vanishing/exploding gradient problems.\n",
    "   - **Leaky**: Like leaky ReLU, it can avoid dead neurons because it's non-zero for negative inputs.\n",
    "   - **Defined for all values**: It's smooth and differentiable everywhere.\n",
    "\n",
    "4. **Activation Functions Use Cases**:\n",
    "   - **SELU**: Good default for deep networks, especially when network architecture supports self-normalization.\n",
    "   - **Leaky ReLU/variants**: Useful in deep networks to avoid dead neurons and combat vanishing gradient issues.\n",
    "   - **ReLU**: A traditional default for many scenarios due to its simplicity.\n",
    "   - **Tanh**: Useful when you need outputs between -1 and 1.\n",
    "   - **Logistic (Sigmoid)**: Mostly used in binary classification problems in the output layer.\n",
    "   - **Softmax**: For multi-class classification tasks in the output layer.\n",
    "\n",
    "5. **High Momentum in SGD**:\n",
    "   If momentum is set too close to 1, the updates can become very large, and the optimizer might overshoot the minimum and diverge, leading to training instability.\n",
    "\n",
    "6. **Sparse Model Production**:\n",
    "   - **Pruning**: Train the model, then remove neurons/weights with the smallest absolute values.\n",
    "   - **Regularization**: Using L1 regularization which encourages weights to become exactly zero.\n",
    "   - **Dropout**: While it doesn't produce a traditionally sparse model, dropout can act like a model with fewer neurons.\n",
    "\n",
    "7. **Dropout and Training/Inference**:\n",
    "   Dropout can slow down convergence during training because it \"turns off\" neurons randomly, making the effective network smaller. However, during inference, dropout is turned off, so it doesn't slow down predictions. **MC Dropout**, on the other hand, involves running inference with dropout multiple times and averaging the results, so it will be slower than standard inference.\n",
    "\n",
    "8. **Deep Neural Network on CIFAR10**:\n",
    "   While I can't directly train a model here, I can provide a blueprint to follow:\n",
    "\n",
    "   a. **DNN with 20 hidden layers**:\n",
    "   ```python\n",
    "   model = keras.models.Sequential()\n",
    "   model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "   for _ in range(20):\n",
    "       model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "   model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "   ```\n",
    "\n",
    "   b. **Nadam & Early Stopping**:\n",
    "   ```python\n",
    "   optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "   early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "   model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "   history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "   ```\n",
    "\n",
    "   c. **Batch Normalization**:\n",
    "   Introduce `BatchNormalization` layers between the dense layers. It can help in faster convergence and potentially better models, but might slightly slow down training due to additional operations.\n",
    "\n",
    "   d. **Replacing with SELU**:\n",
    "   Standardize input features, change the initializer to `lecun_normal`, and replace `elu` with `selu`.\n",
    "\n",
    "   e. **Alpha Dropout & MC Dropout**:\n",
    "   Introduce `AlphaDropout` layers in the network. For MC Dropout, during inference, run the model multiple times with dropout still enabled and average the predictions.\n",
    "\n",
    "Remember, the provided code is a blueprint. Depending on the platform, libraries' versions, and specific requirements, you might need to make adjustments. Always monitor the training process and adjust hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc93ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

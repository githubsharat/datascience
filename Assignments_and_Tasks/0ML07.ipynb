{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352e7937",
   "metadata": {},
   "source": [
    "1. **Target Function**: The target function, often denoted as \\(f\\), represents the true underlying relationship between the inputs and the output in a dataset. In a real-life example, consider predicting the price of a house based on its features. The target function is the true pricing function that considers all factors and perfectly prices a house. To assess its fitness, we compare the predictions of our learned function (hypothesis) with the actual outputs to determine how close they are.\n",
    "  \n",
    "2. **Predictive vs. Descriptive Models**:\n",
    "   - **Predictive Models**: Predict a target value based on input data. Example: Predicting stock prices based on historical data.\n",
    "   - **Descriptive Models**: Describe patterns within data without predicting a specific value. Example: Segmenting customers into different clusters based on their shopping behaviors.\n",
    "   - **Difference**: Predictive models give a specific output (e.g., future sales), while descriptive models provide insights or patterns without a specific prediction.\n",
    "\n",
    "3. **Evaluating Classification Model**:\n",
    "   - **Accuracy**: Proportion of correctly classified instances.\n",
    "   - **Precision**: Proportion of positive identifications that were actually correct.\n",
    "   - **Recall (Sensitivity)**: Proportion of actual positives that were correctly classified.\n",
    "   - **F-measure**: Harmonic mean of precision and recall.\n",
    "   - **ROC Curve**: Plots the true positive rate against the false positive rate.\n",
    "\n",
    "4. **Underfitting, Overfitting, and Bias-Variance Trade-off**:\n",
    "   i. **Underfitting**: When the model is too simple to capture the patterns in the data. It might occur due to a simplistic model or insufficient features.\n",
    "   ii. **Overfitting**: When the model is too complex and fits the noise in the training data. It might happen when a model is trained too long or on a small dataset.\n",
    "   iii. **Bias-Variance Trade-off**: Bias is the error due to overly simplistic assumptions in the learning algorithm. Variance is the error due to too much complexity. Ideally, we want a balance - low bias and low variance.\n",
    "\n",
    "5. **Improving Model Performance**: Yes, you can boost efficiency through feature engineering, adding more data, using more complex models, or tuning hyperparameters.\n",
    "\n",
    "6. **Evaluating Unsupervised Models**: Metrics like Silhouette Score, Davies-Bouldin Index, or purity can be used. It's trickier than supervised methods because there's no \"ground truth.\"\n",
    "\n",
    "7. **Classification and Regression Models**: Typically, classification models are for categorical data, and regression models are for numerical data. Using them interchangeably can lead to issues, but some methods can be adapted, e.g., logistic regression for classification.\n",
    "\n",
    "8. **Predictive Modeling for Numerical Values**: It predicts a continuous value, like house prices. Categorical predictive modeling predicts categories, like \"spam\" or \"not spam.\"\n",
    "\n",
    "9. **Model Metrics Calculation**:\n",
    "   - **Error Rate**: \\( \\frac{3+7}{100} = 0.10 \\) or 10%.\n",
    "   - **Kappa**: A statistic that compares an observed accuracy with an expected accuracy (random chance). Calculation involves a confusion matrix.\n",
    "   - **Sensitivity (Recall)**: \\( \\frac{15}{15+3} = 0.833 \\).\n",
    "   - **Precision**: \\( \\frac{15}{15+7} = 0.682 \\).\n",
    "   - **F-measure**: \\( 2 * \\frac{Precision * Recall}{Precision + Recall} \\).\n",
    "\n",
    "10. **Quick Notes**:\n",
    "   1. **Hold-out Method**: Dividing data into a training set and a test set.\n",
    "   2. **Tenfold Cross-validation**: Dividing data into 10 parts; train on 9 and test on 1, rotate and repeat.\n",
    "   3. **Parameter Tuning**: Adjusting model parameters to improve performance.\n",
    "\n",
    "11. **Definitions**:\n",
    "   1. **Purity vs. Silhouette Width**: Purity measures the extent to which clusters contain single-class samples. Silhouette width measures how similar an object is to its own cluster compared to other clusters.\n",
    "   2. **Boosting vs. Bagging**: Boosting is an ensemble technique that adjusts the weight of an observation based on the last classification. Bagging (Bootstrap Aggregating) involves taking random samples of the dataset.\n",
    "   3. **Eager Learner vs. Lazy Learner**: Eager learners construct a classification model before receiving new data points (e.g., Decision Trees). Lazy learners wait until a new data point is provided, then construct a model or make a classification (e.g., k-NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c2165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

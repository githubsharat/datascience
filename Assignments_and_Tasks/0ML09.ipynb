{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474e66f9",
   "metadata": {},
   "source": [
    "1. **Feature Engineering**: \n",
    "   - Definition: Transforming raw data into a format that makes it easier for ML algorithms to interpret. \n",
    "   - Aspects: Creation of interaction features, polynomial features, binning, encoding categorical variables, normalization, etc.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Definition: Process of selecting a subset of important features.\n",
    "   - Aim: Reduce overfitting, improve model performance, and reduce training time.\n",
    "   - Methods: Filter methods, wrapper methods, and embedded methods.\n",
    "\n",
    "3. **Filter vs. Wrapper**:\n",
    "   - **Filter**: Ranks features based on statistical measures (e.g., chi-squared). \n",
    "     - **Pros**: Faster, less computationally intensive.\n",
    "     - **Cons**: Doesn't consider feature interactions.\n",
    "   - **Wrapper**: Uses ML models to evaluate feature subsets.\n",
    "     - **Pros**: Better performance, considers interactions.\n",
    "     - **Cons**: Computationally expensive.\n",
    "\n",
    "4. \n",
    "   i. **Feature Selection Process**: Rank features based on chosen criteria, select the top-n features or set a threshold.\n",
    "   ii. **Feature Extraction**: Transforms high-dimensional data into a reduced dimensionality while retaining key information. Example: PCA (Principal Component Analysis) reduces data dimensions by finding principal components that maximize variance. Widely used algorithms: PCA, LDA.\n",
    "\n",
    "5. **Text Categorization & Feature Engineering**: Involves tokenization, stop word removal, stemming/lemmatization, and vectorization (like TF-IDF).\n",
    "\n",
    "6. **Cosine Similarity**: \n",
    "   - Ideal for text as it measures the cosine of the angle between two vectors, focusing on orientation, not magnitude. \n",
    "   - Calculation: dot(A, B) / (||A||*||B||). For given vectors, the cosine similarity is approximately 0.94.\n",
    "\n",
    "7. \n",
    "   i. **Hamming Distance**: Counts differing positions in two strings of equal length. For the provided strings, distance = 2.\n",
    "   ii. **Jaccard & SMC**: \n",
    "      - Jaccard = Intersection/Union = 4/8 = 0.5.\n",
    "      - SMC = (Total Matches)/n = 6/8 = 0.75.\n",
    "\n",
    "8. **High-dimensional Dataset**:\n",
    "   - Data with a large number of features/columns.\n",
    "   - Examples: Gene expression data, image pixels.\n",
    "   - **Challenges**: Curse of dimensionality, overfitting.\n",
    "   - **Solution**: Dimensionality reduction, feature selection.\n",
    "\n",
    "9. **Notes**:\n",
    "   1. **PCA**: Transforms data into a new coordinate system by considering the variance.\n",
    "   2. **Support Vectors**: Points in an SVM that lie close to the decision boundary.\n",
    "   3. **Embedded Technique**: Combines qualities of filter & wrapper; uses algorithms (like LASSO) that perform feature selection in the training process.\n",
    "\n",
    "10. **Comparison**:\n",
    "   1. **Sequential Backward Exclusion vs. Sequential Forward Selection**: Exclusion begins with all features and removes iteratively; Selection starts with none and adds features iteratively.\n",
    "   2. **Filter vs. Wrapper**: Filter uses statistical measures; Wrapper evaluates feature subsets using ML models.\n",
    "   3. **SMC vs. Jaccard**: SMC considers both matches and mismatches; Jaccard focuses solely on positive matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

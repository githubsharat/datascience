{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b833adc9",
   "metadata": {},
   "source": [
    "1. **Supervised Learning**: It is a type of machine learning where a model is trained on labeled data. The \"supervised\" name signifies that the learning process is guided or supervised by known outcomes or labels.\n",
    " \n",
    "2. **Hospital Example**: Predicting whether a patient has a particular disease based on their symptoms and medical history.\n",
    "\n",
    "3. **Three Examples**:\n",
    "   - Email spam detection.\n",
    "   - House price prediction based on features.\n",
    "   - Handwritten digit recognition.\n",
    "\n",
    "4. **Classification and Regression**: Classification involves predicting discrete labels (e.g., spam or not-spam), while regression deals with predicting continuous values (e.g., house price).\n",
    "\n",
    "5. **Classification Algorithms**:\n",
    "   - Decision Trees\n",
    "   - Neural Networks\n",
    "   - k-Nearest Neighbors\n",
    "\n",
    "6. **SVM Model**: Support Vector Machine (SVM) is a supervised learning algorithm that finds the best hyperplane to separate data into classes in a high-dimensional space.\n",
    "\n",
    "7. **Cost in SVM**: It refers to the penalty associated with misclassifying data points. It's controlled by the parameter `C` in the SVM. A higher value of `C` means a higher penalty for misclassification.\n",
    "\n",
    "8. **Support Vectors**: These are the data points that lie closest to the decision boundary (or hyperplane) and are crucial in defining it.\n",
    "\n",
    "9. **Kernel in SVM**: It's a function that transforms data into a higher dimension to make it separable.\n",
    "\n",
    "10. **Factors Influencing SVM**:\n",
    "   - Choice of kernel.\n",
    "   - Regularization parameter (C).\n",
    "   - Gamma parameter in radial basis function (RBF) kernel.\n",
    "\n",
    "11. **SVM Benefits**:\n",
    "   - Effective in high-dimensional spaces.\n",
    "   - Works well when the margin of separation is clear.\n",
    "\n",
    "12. **SVM Drawbacks**:\n",
    "   - Sensitive to noise.\n",
    "   - Can be computationally intensive.\n",
    "\n",
    "13. **Notes**:\n",
    "   1. *kNN Validation Flaw*: Without proper validation, kNN can overfit to noise in the training data.\n",
    "   2. *Choosing k in kNN*: A smaller `k` can capture noise, while a larger `k` can smooth out decision boundaries. Often chosen via cross-validation.\n",
    "   3. *Decision Tree Inductive Bias*: Prefers shorter trees over longer ones, and places features with more distinct values at the top.\n",
    "\n",
    "14. **kNN Benefits**:\n",
    "   - Simple to understand and implement.\n",
    "   - Can adapt easily to multi-class classification.\n",
    "\n",
    "15. **kNN Drawbacks**:\n",
    "   - Computationally intensive for large datasets.\n",
    "   - Sensitive to irrelevant or redundant features.\n",
    "\n",
    "16. **Decision Tree**: A flowchart-like tree structure where each internal node represents a feature(or attribute), each branch represents a decision rule, and each leaf node represents an outcome.\n",
    "\n",
    "17. **Node vs. Leaf in Decision Tree**: Node represents a decision or test on an attribute, while a leaf represents a class label or decision result.\n",
    "\n",
    "18. **Entropy in Decision Tree**: It's a measure of randomness or unpredictability in the class label of data.\n",
    "\n",
    "19. **Information Gain**: It's the reduction in entropy achieved by partitioning a dataset based on an attribute.\n",
    "\n",
    "20. **Decision Tree Advantages**:\n",
    "   - Easy to understand and interpret.\n",
    "   - Requires minimal data preprocessing.\n",
    "   - Can handle both numerical and categorical data.\n",
    "\n",
    "21. **Decision Tree Drawbacks**:\n",
    "   - Prone to overfitting.\n",
    "   - Sensitive to small variations in data.\n",
    "   - Can be biased to attributes with more levels.\n",
    "\n",
    "22. **Random Forest**: It's an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e5855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

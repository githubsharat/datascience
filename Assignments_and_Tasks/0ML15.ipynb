{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ec510d",
   "metadata": {},
   "source": [
    "1. **Differences**:\n",
    "   - **Supervised Learning**: Algorithms learn from labeled data, and this learning helps make predictions or infer mappings.\n",
    "   - **Semi-Supervised Learning**: Uses both labeled and unlabeled data for training. Common in situations where acquiring labeled data is expensive.\n",
    "   - **Unsupervised Learning**: Algorithms learn from unlabeled data, often by discovering hidden patterns. \n",
    "\n",
    "2. **Classification Problem Examples**:\n",
    "   - Email Spam Detection: Classifying emails as spam or not.\n",
    "   - Image Recognition: Identifying objects within images.\n",
    "   - Medical Diagnosis: Diagnosing diseases based on symptoms.\n",
    "   - Loan Approval: Deciding to approve or reject a loan based on a person's financial records.\n",
    "   - Sentiment Analysis: Classifying sentiment of text as positive, negative, or neutral.\n",
    "\n",
    "3. **Classification Process Phases**:\n",
    "   - **Data Collection**: Gathering raw data.\n",
    "   - **Data Preparation**: Cleaning and transforming data for analysis.\n",
    "   - **Feature Selection**: Choosing the most relevant variables to improve accuracy.\n",
    "   - **Model Training**: Using algorithms on training data.\n",
    "   - **Model Testing**: Evaluating model's performance on unseen data.\n",
    "   - **Deployment**: Using the model in real-world scenarios.\n",
    "   \n",
    "4. **SVM in-depth**: Support Vector Machines aim to find the optimal hyperplane that separates data into classes. In a 2D space, this is a line. The support vectors are data points that are closest to this hyperplane. SVM uses kernels to handle non-linear separations.\n",
    "\n",
    "5. **SVM Benefits & Drawbacks**:\n",
    "   - **Benefits**: Effective in high-dimensional spaces, versatile through different kernel functions.\n",
    "   - **Drawbacks**: Not suitable for very large datasets, sensitive to noise.\n",
    "\n",
    "6. **kNN in-depth**: The k-Nearest Neighbors algorithm classifies an instance based on the majority class among its 'k' closest instances in the training dataset.\n",
    "\n",
    "7. **kNN's Error & Validation**: Error rate is the fraction of incorrect classifications. Validation error is error rate on a validation dataset that wasn't used in training.\n",
    "\n",
    "8. **Difference Measurement in kNN**: Typically, it's done through techniques like cross-validation. Training error measures performance on training data, while test error measures performance on new data.\n",
    "\n",
    "9. **kNN Algorithm**:\n",
    "   - Choose 'k' and distance metric.\n",
    "   - For a new data point:\n",
    "     - Compute distance to all points in the dataset.\n",
    "     - Select 'k' shortest distances.\n",
    "     - Assign class based on majority among 'k' points.\n",
    "\n",
    "10. **Decision Tree**: It's a flowchart-like structure where decisions are made based on attribute values. Types of nodes:\n",
    "   - **Decision Node**: Tests on attributes.\n",
    "   - **Leaf Node**: Decision result or class label.\n",
    "   \n",
    "11. **Scanning Decision Tree**: Methods include Depth-First Search and Breadth-First Search.\n",
    "\n",
    "12. **Decision Tree Algorithm**: \n",
    "   - Start with the entire dataset.\n",
    "   - Choose an attribute to split on using metrics like information gain.\n",
    "   - Repeat on the sub-datasets until criteria are met (like maximum depth or minimum data points at a leaf).\n",
    "\n",
    "13. **Inductive Bias in Decision Tree**: Prefers shorter trees. To avoid overfitting: prune the tree, set maximum depth, or require a minimum number of instances for splitting.\n",
    "\n",
    "14. **Advantages & Disadvantages**:\n",
    "   - **Advantages**: Easy to understand, can handle both numeric and categorical data.\n",
    "   - **Disadvantages**: Prone to overfitting, can be unstable with small data changes.\n",
    "\n",
    "15. **Problems for Decision Trees**: Suitable for classification and regression problems where relationships between attributes and outcomes are hierarchical or conditional.\n",
    "\n",
    "16. **Random Forest in-depth**: Ensemble method that creates multiple decision trees during training and aggregates their results. \n",
    "   - **Distinct Feature**: Uses bootstrapping to sample data for each tree and selects a subset of features for each split.\n",
    "\n",
    "17. **OOB Error & Variable Importance in Random Forest**:\n",
    "   - **OOB Error**: Error calculated on Out-Of-Bag samples (samples not used during the bootstrapping for a particular tree).\n",
    "   - **Variable Importance**: Measures the importance of each feature based on the average decrease in impurity (like Gini impurity) they cause in the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeebfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbfba04",
   "metadata": {},
   "source": [
    "1. **Dependent vs Independent Variable**: \n",
    "   - **Dependent Variable**: The variable we're trying to predict or explain.\n",
    "   - **Independent Variable**: The variable used to predict or explain the dependent variable.\n",
    "\n",
    "2. **Simple Linear Regression**: It is a linear approach for modeling the relationship between a dependent variable and one independent variable.\n",
    "   - **Example**: Predicting a person's weight based on their height.\n",
    "\n",
    "3. **Slope in Linear Regression**: The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "\n",
    "4. **Slope of the Graph**: Given points (3, 2) and (2, 2), the slope \\( m \\) is:\n",
    "   \\[ m = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{2 - 2}{2 - 3} = 0 \\]\n",
    "   The slope is 0.\n",
    "\n",
    "5. **Positive Slope**: It means that as the independent variable increases, the dependent variable also increases.\n",
    "\n",
    "6. **Negative Slope**: It means that as the independent variable increases, the dependent variable decreases.\n",
    "\n",
    "7. **Multiple Linear Regression**: An extension of simple linear regression that uses two or more independent variables to predict a dependent variable.\n",
    "\n",
    "8. **Sum of Squares Due to Error (SSE)**: The sum of the squares of the differences between the observed values and the values predicted by the model.\n",
    "\n",
    "9. **Sum of Squares Due to Regression (SSR)**: The difference between the total variation in the dependent variable and the sum of squares due to error.\n",
    "\n",
    "10. **Multicollinearity**: Occurs in multiple regression when two or more independent variables are highly correlated, making it difficult to isolate the effect of each variable.\n",
    "\n",
    "11. **Heteroskedasticity**: Refers to the situation where the variability of the error terms/residuals is not constant across all levels of the independent variables.\n",
    "\n",
    "12. **Ridge Regression**: A type of linear regression that includes a regularization term. The regularization term discourages overly complex models which can overfit the data.\n",
    "\n",
    "13. **Lasso Regression**: Another form of linear regression that uses regularization. The key difference from ridge regression is that lasso tends to result in sparser models, setting some coefficients exactly to zero.\n",
    "\n",
    "14. **Polynomial Regression**: A type of regression analysis in which the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an nth degree polynomial.\n",
    "\n",
    "15. **Basis Function**: A set of functions used to represent data or a function in terms of simpler functions. In regression, these can be used to transform the input data to obtain a desired form or shape.\n",
    "\n",
    "16. **Logistic Regression**: A statistical method for analyzing datasets in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (i.e., it will occur or it will not). It estimates the probability that a given instance belongs to a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ceff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

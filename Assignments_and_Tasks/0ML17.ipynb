{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2504972f",
   "metadata": {},
   "source": [
    "1. **Basic Linear Regression**: Basic linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data. The simplest form of the equation is:\n",
    "\\[ y = mx + c \\]\n",
    "   - \\( y \\) is the dependent variable.\n",
    "   - \\( x \\) is the independent variable.\n",
    "   - \\( m \\) is the slope.\n",
    "   - \\( c \\) is the y-intercept.\n",
    "   \n",
    "   A **graph** representing this will be a straight line where the slope defines the steepness of the line and the y-intercept is the point where the line crosses the y-axis.\n",
    "\n",
    "2. **Rise, Run, and Slope on a Graph**:\n",
    "   - **Rise**: It's the difference in the vertical distance (y-values).\n",
    "   - **Run**: It's the difference in the horizontal distance (x-values).\n",
    "   - **Slope**: It's the ratio of the \"rise\" to the \"run\". Formula:\n",
    "     \\[ m = \\frac{rise}{run} \\]\n",
    "\n",
    "3. **Graph to Demonstrate Slope**:\n",
    "   - A straight line moving upwards from left to right indicates a **positive slope**.\n",
    "   - A straight line moving downwards from left to right indicates a **negative slope**.\n",
    "   The steeper the line, the greater the magnitude of the slope.\n",
    "\n",
    "4. **Curve Linear Slope**:\n",
    "   - A **curved line** moving upwards as you move from left to right shows a **curved positive slope**.\n",
    "   - Similarly, a curved line moving downwards as you go from left to right represents a **curved negative slope**.\n",
    "\n",
    "5. **Maximum and Minimum Points on Curves**:\n",
    "   - On a curve, the **highest point** is termed the **maximum**.\n",
    "   - The **lowest point** is called the **minimum**.\n",
    "\n",
    "6. **Formulas for Ordinary Least Squares (OLS)**:\n",
    "   - For the slope \\( m \\):\n",
    "     \\[ m = \\frac{n(\\Sigma xy) - (\\Sigma x)(\\Sigma y)}{n(\\Sigma x^2) - (\\Sigma x)^2} \\]\n",
    "   - For the y-intercept \\( c \\):\n",
    "     \\[ c = \\frac{(\\Sigma y) - m(\\Sigma x)}{n} \\]\n",
    "\n",
    "7. **OLS Algorithm**:\n",
    "   1. For given data points, calculate the sum of x values, y values, the product of x and y, and the square of x.\n",
    "   2. Plug these sums into the formulas for m and c.\n",
    "   3. Using m and c, form the regression line equation.\n",
    "\n",
    "8. **Standard Error of Regression**: Represents the spread between the actual y-values and the predicted y-values. In a graph, this is the vertical distance between a data point and the regression line.\n",
    "\n",
    "9. **Example of Multiple Linear Regression**: Predicting house prices based on the size of the house and the number of rooms. The equation would be like:\n",
    "\\[ Price = a(Size) + b(Rooms) + c \\]\n",
    "\n",
    "10. **Regression Analysis Assumptions & BLUE Principle**:\n",
    "   - Linearity: Relationship between independent and dependent variable is linear.\n",
    "   - Independence: Observations are independent of each other.\n",
    "   - Homoscedasticity: Constant variance of residuals.\n",
    "   - Normality: Residuals should be normally distributed.\n",
    "   \n",
    "   **BLUE**: Best Linear Unbiased Estimator means the method of estimating parameters results in linear estimates which are unbiased and have the lowest variance.\n",
    "\n",
    "11. **Two Major Issues with Regression Analysis**:\n",
    "   - **Multicollinearity**: When two or more independent variables in the regression are highly correlated.\n",
    "   - **Overfitting**: When the model fits too closely to the specificities of the training data and performs poorly on new data.\n",
    "\n",
    "12. **Improving Linear Regression Model Accuracy**: Feature selection, regularization techniques, using interaction terms, and checking & validating assumptions.\n",
    "\n",
    "13. **Polynomial Regression**: Models a nonlinear relationship between the value of x and the corresponding conditional mean of y. Example: Fitting sales based on advertising spend, where the relationship is not strictly linear but more of a curve.\n",
    "\n",
    "14. **Logistic Regression**: Used when the dependent variable is categorical. Predicts the probability of the outcome. For binary outcomes, it predicts the log odds of the outcome.\n",
    "\n",
    "15. **Logistic Regression Assumptions**: No multicollinearity, requires a large sample size, linearity of independent variables and log odds.\n",
    "\n",
    "16. **Maximum Likelihood Estimation (MLE)**: It's a method used to estimate the parameters of a model. In logistic regression, we use MLE to find the best-fitting line. It chooses the values of the parameters to maximize the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65984d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

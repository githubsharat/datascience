{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd532738",
   "metadata": {},
   "source": [
    "1. **Difference between Supervised and Unsupervised Learning**:\n",
    "   - **Supervised Learning**: Uses labeled data to train models. The algorithm learns from the training data to make predictions or infer mappings.\n",
    "     *Example*: Predicting house prices based on features (Regression), classifying emails into spam and not spam (Classification).\n",
    "   - **Unsupervised Learning**: Uses unlabeled data to train models. It aims to infer the underlying structure from the input data.\n",
    "     *Example*: Grouping customers into segments (Clustering), reducing dimensionality of data (Dimensionality Reduction).\n",
    "\n",
    "2. **Unsupervised Learning Applications**:\n",
    "   - Market segmentation for targeted marketing.\n",
    "   - Dimensionality reduction for visualization or compression.\n",
    "   - Recommender systems for suggesting products or content.\n",
    "   - Anomaly detection in credit card transactions.\n",
    "\n",
    "3. **Three Main Types of Clustering Methods**:\n",
    "   - **Partitioning methods**: Divide the data set into non-overlapping subsets. *Example*: k-means.\n",
    "   - **Hierarchical methods**: Produce a nested sequence of clusters. *Example*: Agglomerative clustering.\n",
    "   - **Density-based methods**: Produce arbitrary shaped clusters and can even identify noise. *Example*: DBSCAN.\n",
    "\n",
    "4. **K-means Algorithm and Clustering Consistency**: \n",
    "   The k-means algorithm uses an iterative approach. The centroids of clusters are recalculated in each iteration until they no longer change significantly, ensuring consistency in clustering. The consistency is determined when the sum of the squared distances between data points and their corresponding centroids is minimized.\n",
    "\n",
    "5. **Difference between K-means and K-medoids**:\n",
    "   - **K-means**: Centroids are the mean value of the points within a cluster.\n",
    "   - **K-medoids**: The centroid is an actual data point in the cluster, making it more robust to outliers.\n",
    "   *Illustration*: Imagine three data points in a 1D space: 1, 2, and 10. The k-means centroid would be (1+2+10)/3 = 4.33, while the k-medoids centroid would be 2 (since it's the actual data point that's most centrally located).\n",
    "\n",
    "6. **Dendrogram**: \n",
    "   A dendrogram is a tree-like diagram that displays the sequence of merges or splits of hierarchical clustering. It helps in understanding the arrangement of clusters and deciding the number of clusters. \n",
    "   **How to use**: By drawing a horizontal line across the dendrogram, the number of vertical lines it intersects indicates the number of clusters at that height.\n",
    "\n",
    "7. **SSE (Sum of Squared Errors)**: \n",
    "   It is the sum of the squared distance between each data point and its cluster centroid. In k-means, the aim is to minimize the SSE, resulting in tighter clusters.\n",
    "\n",
    "8. **K-means Algorithm**:\n",
    "   1. Choose 'k' initial centroids (randomly selected from the data points).\n",
    "   2. Assign each data point to the nearest centroid.\n",
    "   3. Recalculate the centroid of each cluster based on the assigned data points.\n",
    "   4. Repeat steps 2-3 until centroids do not change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "9. **Single Link vs. Complete Link in Hierarchical Clustering**:\n",
    "   - **Single Link**: The distance between two clusters is defined as the shortest distance between points in the two clusters.\n",
    "   - **Complete Link**: The distance between two clusters is defined as the longest distance between points in the two clusters.\n",
    "\n",
    "10. **Apriori Concept in Market Basket Analysis**:\n",
    "   The Apriori principle states that if an itemset is frequent, then all of its subsets must also be frequent. This helps in reducing the number of candidate itemsets.\n",
    "   *Example*: If {apple, banana} is infrequent, then {apple, banana, cherry} will also be infrequent and won't need further consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145bca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

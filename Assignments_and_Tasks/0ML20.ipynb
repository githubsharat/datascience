{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20658479",
   "metadata": {},
   "source": [
    "1. **Support Vector Machines (SVM)**:\n",
    "SVM is a supervised machine learning algorithm that aims to find the hyperplane that best separates data into classes in such a way that the margin between the two classes is maximized. For non-linearly separable data, it uses kernel trick to transform the data into a higher dimension to make it linearly separable.\n",
    "\n",
    "2. **Concept of a Support Vector**:\n",
    "Support vectors are the data points that are closest to the hyperplane and influence its orientation and position. They lie along the margin boundaries. If these points are moved or removed, the position of the hyperplane would change.\n",
    "\n",
    "3. **Scaling in SVMs**:\n",
    "Scaling is essential in SVMs to avoid features with broader ranges dominating the distance metric, especially since SVMs rely on calculating distances between instances.\n",
    "\n",
    "4. **SVM Classifier Outputs**:\n",
    "Typically, SVMs do not output probabilities for each class. However, they can produce confidence scores, which are distances between test samples and the hyperplane. Some SVM implementations can estimate class probabilities using techniques like Platt scaling, but this requires additional cross-validation.\n",
    "\n",
    "5. **Primal vs. Dual in SVM**:\n",
    "For a dataset with millions of instances and hundreds of features, using the primal form is generally more suitable. The dual form is computationally efficient when the number of instances (samples) is less than the number of features.\n",
    "\n",
    "6. **Tuning RBF Kernel in SVM**:\n",
    "If an SVM with an RBF kernel underfits the training set:\n",
    "- Increase \\( \\gamma \\) (or decrease it if there's overfitting).\n",
    "- Increase the regularization parameter \\( C \\) if the model underfits or decrease it if the model overfits.\n",
    "\n",
    "7. **Setting QP parameters**:\n",
    "For a soft margin linear SVM classifier:\n",
    "- \\( H \\) is the matrix with size m x m (where m is number of features), which will be a zero matrix except the top-left corner which has an identity matrix (of size n x n, n being number of features) for linear SVMs.\n",
    "- \\( f \\) is a vector filled with values equal to the penalty parameter \\( C \\).\n",
    "- \\( A \\) is a matrix with negative identity matrix on top of identity matrix.\n",
    "- \\( b \\) is a vector with the first half filled with 0 and the second half filled with the penalty parameter \\( C \\).\n",
    "\n",
    "8. **Training LinearSVC, SVC, SGDClassifier**:\n",
    "By training LinearSVC, SVC with a linear kernel, and SGDClassifier with hinge loss on a linearly separable dataset, you should get similar models, provided hyperparameters like regularization are set similarly and models are sufficiently trained.\n",
    "\n",
    "9. **SVM on MNIST**:\n",
    "Training an SVM on the MNIST dataset requires a one-vs-the-rest (or one-vs-all) strategy since SVMs are inherently binary. Due to the size of the dataset, grid search or random search on smaller validation sets can be used for hyperparameter tuning. Achievable accuracy depends on the parameter settings, but with proper tuning, SVMs can achieve high accuracy on MNIST, often above 98%.\n",
    "\n",
    "10. **SVM Regressor on California Housing**:\n",
    "SVM can also be used for regression (SVR). For the California housing dataset, you would typically preprocess the data, scale the features, and then train an SVM regressor. Hyperparameter tuning, using techniques like grid search or random search, would be needed to optimize the model's performance. The effectiveness of the model would depend on the chosen kernel, hyperparameters, and the preprocessing steps taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e3165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

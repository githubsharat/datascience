{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243673aa",
   "metadata": {},
   "source": [
    "1. **Estimated Depth of a Decision Tree**:\n",
    "In a binary tree, the maximum depth \\( d \\) (unrestricted) can be estimated as \\( \\log_2(n) \\), where \\( n \\) is the number of instances. So for 1 million instances, the estimated depth would be \\( \\log_2(1,000,000) \\) which is roughly 20. However, in practice, it might be less because of early stopping conditions or the fact that not every instance will require a leaf.\n",
    "\n",
    "2. **Gini Impurity**:\n",
    "Typically, the Gini impurity of a child node is lower than its parent because the purpose of the split is to clarify the classes. If a split is made, it's because it's believed that this split can better separate the instances. However, it's not guaranteed to always be lower. It's just usually the case.\n",
    "\n",
    "3. **Reducing max_depth for Overfitting**:\n",
    "Yes, if a Decision Tree is overfitting the training set, reducing the maximum depth (`max_depth`) can help. This makes the tree more general and prevents it from fitting to the noise in the training data.\n",
    "\n",
    "4. **Scaling Input Features for Decision Trees**:\n",
    "Decision Trees are not sensitive to the scale of input features. Therefore, scaling the input features will not have any effect on its performance, whether it's underfitting or overfitting.\n",
    "\n",
    "5. **Training Time Estimate**:\n",
    "The training time complexity of a Decision Tree is \\( O(n \\times m \\log(m)) \\), where \\( n \\) is the number of features and \\( m \\) is the number of instances. If you multiply \\( m \\) by 10, the training time will be multiplied roughly by \\( 10 \\log(10) \\) which is about 23. Therefore, if it takes 1 hour for 1 million instances, it might take roughly 23 hours for 10 million instances.\n",
    "\n",
    "6. **Presort for Training Speed**:\n",
    "Setting `presort=True` can speed up training on smaller datasets or for smaller depths, but for larger datasets like 100,000 instances, it can considerably slow down training.\n",
    "\n",
    "7. **Training and Fine-tuning on Moons Dataset**:\n",
    "I won't be able to execute the code here directly, but I can guide you step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# a\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# b\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# c\n",
    "params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "\n",
    "# d\n",
    "y_pred = grid_search_cv.best_estimator_.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f697e1",
   "metadata": {},
   "source": [
    "8. **Growing a Forest**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy.stats import mode\n",
    "\n",
    "# a\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))\n",
    "\n",
    "# b\n",
    "forest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(np.mean(accuracy_scores))\n",
    "\n",
    "# c\n",
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)\n",
    "\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n",
    "\n",
    "# d\n",
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c0092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
